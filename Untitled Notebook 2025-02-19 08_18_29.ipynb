{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed273c9-8af9-42e0-a723-7a5a2e778863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!sh scripts/SegRNN/etth1.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b79565-1f42-4595-b112-6d391d42de49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = \"8:26:49\"\n",
    "tid_CPU = \"2h 12m\"\n",
    "tid_GPU = \"8m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba79edf5-d3b1-468f-9b43-4016430cd634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!sh scripts/SegRNN/custom_V2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6bfbab8-496d-47f4-8dbd-7882caa4b3b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hej = \"hej\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c8d99b4-74b9-4df2-8479-396b6af502af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('/Workspace/Users/raha@verdo.com/SegRNN/results/customv2_720_96_SegRNN_Dataset_CustomV2_ft4_sl720_pl96_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4263a1c-2b1d-4663-b6e5-94953bef8ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd2f015-66f1-4360-b0cd-28c03eb0ff32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from data_provider.data_loader import Dataset_CustomV2  # Import your dataloader class\n",
    "\n",
    "# Define the dataset instance (Make sure to match the arguments with what was used during training)\n",
    "import numpy as np\n",
    "predictions = np.load('/Workspace/Users/raha@verdo.com/SegRNN/results/customv2_720_96_SegRNN_Dataset_CustomV2_ft4_sl720_pl96_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy')\n",
    "\n",
    "dataset = Dataset_CustomV2(\n",
    "    root_path='./dataset/',\n",
    "    data_path='data.csv',\n",
    "    flag='train',  # Use the correct split\n",
    "    features='4',\n",
    "    target=['Hz', 'LzNord', 'LzSyd', 'Drb'],\n",
    "    scale=True,\n",
    "    timeenc=0, \n",
    "    freq='h'\n",
    ")\n",
    "\n",
    "# Apply the inverse transform to get human-readable values\n",
    "real_values = dataset.inverse_transform(predictions.reshape(-1, len(dataset.target)))\n",
    "real_values = real_values.reshape(predictions.shape[0], predictions.shape[1], -1)\n",
    "\n",
    "# Print the first few converted predictions\n",
    "print(real_values[:5])  # Check the first 5 prediction rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b241b1-ebfb-4d13-9d97-869aeda56f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_provider.data_loader import Dataset_CustomV2\n",
    "\n",
    "# ‚úÖ Load predictions\n",
    "predictions = np.load('/Workspace/Users/raha@verdo.com/SegRNN/results/customv2_720_96_SegRNN_Dataset_CustomV2_ft4_sl720_pl96_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy')\n",
    "\n",
    "# ‚úÖ Instantiate the dataset (ensuring it uses the same scaler as in training)\n",
    "dataset = Dataset_CustomV2(\n",
    "    root_path='./dataset/',  \n",
    "    data_path='data.csv',  \n",
    "    flag='train',  # Just to load the scaler\n",
    "    features='4',  \n",
    "    target=['Hz', 'LzNord', 'LzSyd', 'Drb'],  \n",
    "    scale=True,  \n",
    "    timeenc=0,  \n",
    "    freq='h'  \n",
    ")\n",
    "\n",
    "# ‚úÖ Get the scaler (trained on full input feature set)\n",
    "scaler = dataset.scaler\n",
    "\n",
    "# ‚úÖ Create a fake input with the same shape the scaler expects\n",
    "fake_input = np.zeros((predictions.shape[0] * predictions.shape[1], scaler.mean_.shape[0]))\n",
    "\n",
    "# ‚úÖ Insert the predictions into the right columns (assuming targets are last)\n",
    "target_indices = [-4, -3, -2, -1]  # Adjust based on your dataset order\n",
    "fake_input[:, target_indices] = predictions.reshape(-1, len(dataset.target))\n",
    "\n",
    "# ‚úÖ Apply inverse transform on the **full data** and extract only target columns\n",
    "unscaled = scaler.inverse_transform(fake_input)\n",
    "real_values = unscaled[:, target_indices]  # Extract only the target columns\n",
    "print(real_values.shape)\n",
    "\n",
    "# ‚úÖ Reshape back to match original shape\n",
    "real_values = real_values.reshape(predictions.shape[0], predictions.shape[1], -1)\n",
    "\n",
    "# ‚úÖ **Print first few human-readable predictions**\n",
    "print(\"First 5 Forecasted Values (Original Scale):\")\n",
    "print(real_values[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a0999c-18f3-492c-8104-aac0b85b132b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_provider.data_loader import Dataset_CustomV2  \n",
    "\n",
    "# ‚úÖ Load Predictions\n",
    "predictions = np.load('/Workspace/Users/raha@verdo.com/SegRNN/results/customv2_720_96_SegRNN_Dataset_CustomV2_ft4_sl720_pl96_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy')\n",
    "\n",
    "# ‚úÖ Debugging Step 1: Check Shape\n",
    "print(f\"üìå Predictions Shape: {predictions.shape}\")  # Should be (batch_size, pred_len, num_targets)\n",
    "\n",
    "# ‚úÖ Debugging Step 2: Print Last 5 Predictions Before Scaling\n",
    "print(\"\\nüöÄ Last 5 Predictions (Before Scaling):\")\n",
    "print(predictions[-5:])\n",
    "\n",
    "# ‚úÖ Create Dataset Instance\n",
    "dataset = Dataset_CustomV2(\n",
    "    root_path='./dataset/',\n",
    "    data_path='data.csv',\n",
    "    flag='train',\n",
    "    features='4',\n",
    "    target=['Hz', 'LzNord', 'LzSyd', 'Drb'],\n",
    "    scale=True,\n",
    "    timeenc=0, \n",
    "    freq='h'\n",
    ")\n",
    "\n",
    "# ‚úÖ Debugging Step 3: Check Inverse Transform\n",
    "# ‚úÖ Extract only target variables\n",
    "num_targets = len(dataset.target)  # Should be 4\n",
    "\n",
    "try:\n",
    "    real_values = dataset.inverse_transform(predictions.reshape(-1, num_targets))  # Apply scaling only on target variables\n",
    "    real_values = real_values.reshape(predictions.shape[0], predictions.shape[1], num_targets)\n",
    "\n",
    "    print(\"\\nüìä Last 5 Forecasted Values (After Scaling):\")\n",
    "    print(real_values[-5:])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error in inverse_transform: {e}\")\n",
    "\n",
    "\n",
    "    # ‚úÖ Debugging Step 4: Print Last 5 Predictions After Scaling\n",
    "    print(\"\\nüìä Last 5 Forecasted Values (After Scaling):\")\n",
    "    print(real_values[-5:])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error in inverse_transform: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd0453ff-90cd-4eae-b44e-99efe8215c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# üìå Load the dataset (Ensure path is correct)\n",
    "file_path = \"./dataset/data.csv\"  # Update path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Apply the same renaming logic from `Dataset_CustomV2`\n",
    "df[\"LzSyd\"] = df[\"PRODUCTION_5XE10B001YJ01\"] + df[\"PRODUCTION_1YC22U001ZT00\"]\n",
    "df.drop(columns=[\"PRODUCTION_5XE10B001YJ01\", \"PRODUCTION_1YC22U001ZT00\"], inplace=True)\n",
    "df.rename(columns={\n",
    "    \"PRODUCTION_1YA22U001ZT00\": \"Hz\",\n",
    "    \"PRODUCTION_1YB22U001ZT00\": \"LzNord\",\n",
    "    \"PRODUCTION_1YD22U001ZT00\": \"Drb\"\n",
    "}, inplace=True)\n",
    "\n",
    "# üéØ Extract only the target columns\n",
    "target_columns = [\"Hz\", \"LzNord\", \"LzSyd\", \"Drb\"]\n",
    "df_targets = df[target_columns]\n",
    "\n",
    "# ‚úÖ Save the extracted targets as a new CSV file\n",
    "df_targets.to_csv(\"./dataset/extracted_targets.csv\", index=False)\n",
    "\n",
    "# üìä Display first few rows\n",
    "print(\"üéØ First 5 Extracted Target Values:\")\n",
    "print(df_targets.head())\n",
    "\n",
    "# Optionally, display summary statistics\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "print(df_targets.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d188ab6e-6a63-4c3f-ab7e-f5c81e0c3004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# üìå Load extracted ground truth values\n",
    "ground_truth_path = \"./dataset/extracted_targets.csv\"\n",
    "df_true = pd.read_csv(ground_truth_path)\n",
    "\n",
    "# üìå Load predictions (raw scaled values)\n",
    "predictions_path = \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2_720_96_SegRNN_Dataset_CustomV2_ft4_sl720_pl96_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy\"  # Adjust path if needed\n",
    "pred_scaled = np.load(predictions_path)\n",
    "\n",
    "# ‚úÖ Load dataset instance (to get correct scaler)\n",
    "dataset = Dataset_CustomV2(\n",
    "    root_path=\"./dataset/\",\n",
    "    data_path=\"data.csv\",\n",
    "    flag=\"train\",\n",
    "    features=\"4\",\n",
    "    target=[\"Hz\", \"LzNord\", \"LzSyd\", \"Drb\"],  # Ensure target matches your model's target outputs\n",
    "    scale=True,\n",
    "    timeenc=0,\n",
    "    freq=\"h\"\n",
    ")\n",
    "\n",
    "# ‚úÖ **Fix inverse transform to apply only on target variables**\n",
    "scaler = dataset.scaler  # Get the fitted scaler\n",
    "\n",
    "# üéØ Extract only target indices from scaler\n",
    "target_indices = [dataset.data_x.shape[1] - len(dataset.target) + i for i in range(len(dataset.target))]\n",
    "\n",
    "# üîπ Create a dummy matrix to inverse transform\n",
    "dummy = np.zeros((pred_scaled.shape[0] * pred_scaled.shape[1], scaler.mean_.shape[0]))  # (samples, total features)\n",
    "dummy[:, target_indices] = pred_scaled.reshape(-1, len(dataset.target))\n",
    "\n",
    "# üîπ Apply inverse transformation\n",
    "dummy = scaler.inverse_transform(dummy)\n",
    "\n",
    "# üéØ Extract only target columns\n",
    "pred_unscaled = dummy[:, target_indices].reshape(pred_scaled.shape[0], pred_scaled.shape[1], -1)\n",
    "\n",
    "# ‚úÖ Ensure matching lengths for comparison\n",
    "df_true = df_true.iloc[-pred_unscaled.shape[0]:]  # Align last predictions\n",
    "\n",
    "# Convert to NumPy array for metric calculation\n",
    "true_values = df_true.values\n",
    "pred_values = pred_unscaled[:, -1, :]  # Take last predicted timestep\n",
    "\n",
    "# üìä Compute Metrics\n",
    "mae = mean_absolute_error(true_values, pred_values)\n",
    "rmse = np.sqrt(mean_squared_error(true_values, pred_values))\n",
    "mape = np.mean(np.abs((true_values - pred_values) / true_values)) * 100\n",
    "r2 = r2_score(true_values, pred_values)\n",
    "\n",
    "# üìù Print Results\n",
    "print(f\"üìä MAE: {mae:.2f}\")\n",
    "print(f\"üìä RMSE: {rmse:.2f}\")\n",
    "print(f\"üìä MAPE: {mape:.2f}%\")\n",
    "print(f\"üìä R¬≤ Score: {r2:.2f}\")\n",
    "\n",
    "# üîπ Plot Predictions vs Ground Truth\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, col in enumerate([\"Hz\", \"LzNord\", \"LzSyd\", \"Drb\"]):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(true_values[:, i], label=\"Ground Truth\", color=\"blue\", alpha=0.7)\n",
    "    plt.plot(pred_values[:, i], label=\"Prediction\", color=\"red\", linestyle=\"dashed\", alpha=0.7)\n",
    "    plt.title(f\"{col} - Forecast vs Actual\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15d80a0-546d-4e23-8d6d-f20f7858c77d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# üéØ Select the last sequence of predictions\n",
    "last_pred = pred_unscaled[-1]  # Last predicted sequence (shape: [96, 4])\n",
    "last_true = true_values[-96:]  # Last matching ground truth values (shape: [96, 4])\n",
    "\n",
    "# üéØ Define target variable names\n",
    "target_names = [\"Hz\", \"LzNord\", \"LzSyd\", \"Drb\"]\n",
    "\n",
    "# üìä Plot the last prediction sequence vs actual values\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, target in enumerate(target_names):\n",
    "    plt.subplot(2, 2, i + 1)  # Create a 2x2 subplot\n",
    "    plt.plot(last_true[:, i], label=\"Ground Truth\", color=\"blue\", alpha=0.7)\n",
    "    plt.plot(last_pred[:, i], label=\"Prediction\", linestyle=\"dashed\", color=\"red\", alpha=0.7)\n",
    "    plt.title(f\"{target} - Last Forecast vs Actual\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907106f5-2d84-41e2-bcac-38f9e19712e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "predictions_path_96 = \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2NOSCALE_720_96_SegRNN_Dataset_CustomV2_ft4_sl720_pl96_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy\"\n",
    "predictions_path_192 = \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2NOSCALE_720_192_SegRNN_Dataset_CustomV2_ft4_sl720_pl\"\n",
    "predictions_path_336 = \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2NOSCALE_720_336_SegRNN_Dataset_CustomV2_ft4_sl720_pl\"\n",
    "predictions_path_720 = \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2NOSCALE_720_720_SegRNN_Dataset_CustomV2_ft4_sl720_pl\"\n",
    "\n",
    "pred_96 = np.load(predictions_path_96)\n",
    "\n",
    "ground_truth_path = \"./dataset/extracted_targets.csv\"\n",
    "df_true = pd.read_csv(ground_truth_path)\n",
    "\n",
    "true_values = df_true.values\n",
    "#pred_values = pred_96[:, -1, :]  # Take last predicted timestep\n",
    "\n",
    "# üéØ Select the last sequence of predictions\n",
    "last_pred = pred_values[-96:]  # Last predicted sequence (shape: [96, 4])\n",
    "last_true = true_values[-96:]  # Last matching ground truth values (shape: [96, 4])\n",
    "\n",
    "print(last_pred.shape)\n",
    "print(last_true.shape)\n",
    "\n",
    "# üéØ Define target variable names\n",
    "target_names = [\"Hz\", \"LzNord\", \"LzSyd\", \"Drb\"]\n",
    "\n",
    "# üìä Plot the last prediction sequence vs actual values\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, target in enumerate(target_names):\n",
    "    plt.subplot(2, 2, i + 1)  # Create a 2x2 subplot\n",
    "    plt.plot(last_true[:, i], label=\"Ground Truth\", color=\"blue\", alpha=0.7)\n",
    "    plt.plot(last_pred[:, i], label=\"Prediction\", linestyle=\"dashed\", color=\"red\", alpha=0.7)\n",
    "    plt.title(f\"{target} - Last Forecast vs Actual\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5eb95d-06ff-4d99-891e-4f5dcf743f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Define prediction file paths\n",
    "predictions_paths = {\n",
    "    96: \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2NOSCALE_720_96_SegRNN_Dataset_CustomV2_ft4_sl720_pl96_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy\",\n",
    "    192: \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2NOSCALE_720_192_SegRNN_Dataset_CustomV2_ft4_sl720_pl192_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy\",\n",
    "    336: \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2NOSCALE_720_336_SegRNN_Dataset_CustomV2_ft4_sl720_pl336_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy\",\n",
    "    720: \"/Workspace/Users/raha@verdo.com/SegRNN/results/customv2NOSCALE_720_720_SegRNN_Dataset_CustomV2_ft4_sl720_pl720_dm512_dr0.5_rtgru_dwpmf_sl48_mae_test_0/pred.npy\",\n",
    "}\n",
    "\n",
    "# Load ground truth values\n",
    "ground_truth_path = \"./dataset/extracted_targets.csv\"\n",
    "df_true = pd.read_csv(ground_truth_path)\n",
    "true_values = df_true.values  # Ensure this aligns with your predictions\n",
    "\n",
    "# Define target variable names\n",
    "target_names = [\"Hz\", \"LzNord\", \"LzSyd\", \"Drb\"]\n",
    "\n",
    "# Iterate over different prediction horizons\n",
    "for horizon, path in predictions_paths.items():\n",
    "    print(f\"\\nüîπ Processing Forecast Horizon: {horizon} timesteps\")\n",
    "\n",
    "    # Load predictions\n",
    "    pred_values = np.load(path)\n",
    "    pred_values = pred_values[:, -1, :]  # Take the last predicted timestep\n",
    "\n",
    "    # Ensure alignment of predictions and ground truth\n",
    "    aligned_true_values = true_values[-pred_values.shape[0]:]\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(aligned_true_values, pred_values)\n",
    "    rmse = np.sqrt(mean_squared_error(aligned_true_values, pred_values))\n",
    "    mape = np.mean(np.abs((aligned_true_values - pred_values) / aligned_true_values)) * 100\n",
    "    r2 = r2_score(aligned_true_values, pred_values)\n",
    "\n",
    "    # Print Metrics\n",
    "    print(f\"üìä MAE: {mae:.2f}\")\n",
    "    print(f\"üìä RMSE: {rmse:.2f}\")\n",
    "    print(f\"üìä MAPE: {mape:.2f}%\")\n",
    "    print(f\"üìä R¬≤ Score: {r2:.2f}\")\n",
    "\n",
    "    # üéØ Select the last `horizon` predictions for visualization\n",
    "    last_pred = pred_values[-horizon:]\n",
    "    last_true = aligned_true_values[-horizon:]\n",
    "\n",
    "    # üìä Plot Predictions vs Ground Truth\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, target in enumerate(target_names):\n",
    "        plt.subplot(2, 2, i + 1)  # 2x2 subplot grid\n",
    "        plt.plot(last_true[:, i], label=\"Ground Truth\", color=\"blue\", alpha=0.7)\n",
    "        plt.plot(last_pred[:, i], label=\"Prediction\", linestyle=\"dashed\", color=\"red\", alpha=0.7)\n",
    "        plt.title(f\"{target} - Last {horizon} Steps Forecast vs Actual\")\n",
    "        plt.xlabel(\"Time Steps\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43bfd4fe-810b-4d76-afc5-3b48c077e408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-02-19 08_18_29",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
